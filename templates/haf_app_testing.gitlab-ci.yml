# HAF Application Testing Templates
#
# Provides reusable CI patterns for HAF-dependent applications.
# These templates standardize common patterns found across HAfAH,
# reputation_tracker, balance_tracker, and hivemind.
#
# Usage in your .gitlab-ci.yml:
#   include:
#     - project: 'hive/common-ci-configuration'
#       ref: develop
#       file: '/templates/haf_app_testing.gitlab-ci.yml'
#
#   detect_changes:
#     extends: .haf_app_detect_changes
#     variables:
#       HAF_APP_CACHE_TYPE: "haf_myapp_sync"
#
#   my_test_job:
#     extends: .haf_app_dind_test_base
#     script:
#       - pytest tests/

variables:
  # Docker image versions - use specific commit with util-linux for NFS flock support
  # Override these if you need a different version
  DOCKER_BUILDER_TAG: "3dd346380da7f28cdaf3ef3955559c905d94bd1c"
  DOCKER_DIND_TAG: "3dd346380da7f28cdaf3ef3955559c905d94bd1c"

  # Cache manager configuration
  CACHE_MANAGER: "/tmp/cache-manager.sh"
  CACHE_MANAGER_REF: "develop"
  # Test cache extraction script
  EXTRACT_TEST_CACHE: "/tmp/extract-test-cache.sh"

  # NFS and local cache paths (standard across all builders)
  DATA_CACHE_NFS_PREFIX: "/nfs/ci-cache"
  DATA_CACHE_HAF_PREFIX: "/cache/replay_data_haf"
  DATA_CACHE_LOCAL_PREFIX: "/cache"

  # Common skip patterns for change detection
  # Files matching these patterns don't require full HAF sync
  HAF_APP_SKIP_PATTERNS: '^tests/|^docs/|\.md$|^README|^CHANGELOG|^LICENSE|^CLAUDE|^\.gitlab-ci'

  # Default timeouts (can be overridden per job)
  HAF_SYNC_TIMEOUT: "7200"    # 2 hours for sync jobs
  HAF_TEST_TIMEOUT: "1800"    # 30 minutes for test jobs
  HAF_EXTRACT_TIMEOUT: "300"  # 5 minutes for cache extraction

include:
  - local: templates/base.gitlab-ci.yml
  - local: templates/docker_image_jobs.gitlab-ci.yml

# =============================================================================
# FETCH STRATEGY SUPPORT
# =============================================================================
# Templates for using GIT_STRATEGY=fetch instead of clone.
# Fetch strategy reduces GitLab server load by reusing workspace between jobs,
# but requires manual submodule handling and corruption cleanup.
#
# Usage:
#   # In your .gitlab-ci.yml variables section, add fetch strategy defaults:
#   variables:
#     GIT_STRATEGY: fetch
#     GIT_DEPTH: 0
#     GIT_SUBMODULE_STRATEGY: none
#     # ... other fetch settings from .haf_fetch_strategy_variables
#
#   # Add corruption cleanup hook at default level:
#   default:
#     hooks: !reference [.haf_git_corruption_cleanup, hooks]
#
#   # In jobs that need submodules, prepend initialization:
#   my_job:
#     before_script:
#       - !reference [.haf_submodule_init, script]
#       - # ... rest of your before_script

# Variables for fetch strategy (copy these to your variables section)
# These are provided as documentation - GitLab doesn't support extending variables
.haf_fetch_strategy_variables:
  variables:
    # Fetch strategy reuses workspace between jobs, reducing GitLab server load.
    # Full clone (depth 0) enables efficient incremental fetches.
    GIT_STRATEGY: fetch
    GIT_DEPTH: 0
    # Disable automatic submodule fetching - we handle it manually
    # This is needed because HAF contains nested hive submodule with relative URLs
    GIT_SUBMODULE_STRATEGY: none
    GIT_SUBMODULE_FORCE_HTTPS: "true"
    # Allow file:// protocol for nested submodules (required for test-tools in hive)
    GIT_CONFIG_COUNT: 1
    GIT_CONFIG_KEY_0: "protocol.file.allow"
    GIT_CONFIG_VALUE_0: "always"
    # Separate clone path prevents clone-strategy jobs from erasing fetch workspaces
    GIT_CLONE_PATH: $CI_BUILDS_DIR/fetch/$CI_RUNNER_SHORT_TOKEN/$CI_CONCURRENT_ID/$CI_PROJECT_PATH

# Git corruption cleanup hook for fetch strategy
# Cleans corrupt git state left by cancelled pipelines (see GitLab #296638, #4600)
# Usage: default: hooks: !reference [.haf_git_corruption_cleanup, hooks]
.haf_git_corruption_cleanup:
  hooks:
    pre_get_sources_script:
      - |
        (
        cd "${CI_PROJECT_DIR:-/builds}" 2>/dev/null || exit 0
        echo "pre_get_sources: checking $(pwd) for corrupt git state"
        if [ -d ".git" ]; then
          # Remove stale lock files that block git operations
          find .git -name "*.lock" -delete 2>/dev/null || true

          # Check if main repo is corrupt - if so, remove .git to force fresh clone
          if ! git rev-parse HEAD >/dev/null 2>&1; then
            echo "pre_get_sources: main repository corrupt, forcing fresh clone"
            rm -rf .git
          else
            # Main repo OK - check and clean corrupt submodules
            if [ -f ".gitmodules" ]; then
              git config --file .gitmodules --get-regexp path 2>/dev/null | awk '{print $2}' | while read submod; do
                needs_clean=false
                [ -z "$submod" ] && continue
                # Check if submodule working directory exists but is corrupt
                if [ -d "$submod" ] && [ -f "$submod/.git" ]; then
                  if ! git -C "$submod" rev-parse HEAD >/dev/null 2>&1; then
                    needs_clean=true
                  fi
                fi
                # Check if .git/modules exists but is corrupt (even if working dir is gone)
                if [ -d ".git/modules/$submod" ]; then
                  if ! git --git-dir=".git/modules/$submod" rev-parse HEAD >/dev/null 2>&1; then
                    echo "pre_get_sources: $submod corrupt (rev-parse failed)"
                    needs_clean=true
                  fi
                fi
                if [ "$needs_clean" = true ]; then
                  echo "pre_get_sources: cleaning corrupt submodule: $submod"
                  rm -rf "$submod" ".git/modules/$submod"
                fi
              done
            fi
            echo "pre_get_sources: existing repo OK"
          fi
        else
          echo "pre_get_sources: no .git directory (fresh workspace)"
        fi
        )

# HAF submodule initialization for fetch strategy
# Manually clones and checks out HAF submodule with nested hive submodules
# Required because GIT_SUBMODULE_STRATEGY: none disables automatic handling
# Usage: before_script: - !reference [.haf_submodule_init, script]
.haf_submodule_init:
  script:
    - |
      if command -v git >/dev/null 2>&1; then
        echo -e "\e[0Ksection_start:$(date +%s):init_submodules[collapsed=true]\r\e[0KInitializing submodules..."
        git config --global --add safe.directory '*'
        # Get the commit we need from the haf submodule
        HAF_URL="https://gitlab.syncad.com/hive/haf.git"
        HAF_SUBMOD_COMMIT=$(git ls-tree HEAD haf | awk '{print $3}')
        echo "Need haf submodule at commit $HAF_SUBMOD_COMMIT"
        # Remove existing haf directory if it exists (can be stale from GIT_STRATEGY=fetch)
        sudo rm -rf haf 2>/dev/null || rm -rf haf
        # Clone and checkout specific commit
        git clone --no-checkout "$HAF_URL" haf
        cd haf
        git fetch origin develop
        git checkout $HAF_SUBMOD_COMMIT
        # Initialize nested submodules inside haf (hive and its submodules)
        git submodule update --init --recursive --jobs 4
        cd ..
        echo -e "\e[0Ksection_end:$(date +%s):init_submodules\r\e[0K"
      fi

# Combined fetch strategy defaults (for reference/documentation)
# Shows all the pieces needed for fetch strategy in one place
.haf_fetch_strategy_example:
  variables:
    GIT_STRATEGY: fetch
    GIT_DEPTH: 0
    GIT_SUBMODULE_STRATEGY: none
    GIT_SUBMODULE_FORCE_HTTPS: "true"
    GIT_CONFIG_COUNT: 1
    GIT_CONFIG_KEY_0: "protocol.file.allow"
    GIT_CONFIG_VALUE_0: "always"
    GIT_CLONE_PATH: $CI_BUILDS_DIR/fetch/$CI_RUNNER_SHORT_TOKEN/$CI_CONCURRENT_ID/$CI_PROJECT_PATH
  hooks: !reference [.haf_git_corruption_cleanup, hooks]
  before_script:
    - !reference [.haf_submodule_init, script]

# =============================================================================
# CACHE MANAGER SETUP
# =============================================================================
# Standardized cache-manager.sh fetching that all HAF apps should use.
# This ensures consistent caching behavior across all applications.

.fetch_cache_manager:
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):cache_manager_setup[collapsed=true]\r\e[0KSetting up cache-manager..."
      mkdir -p "$(dirname "$CACHE_MANAGER")"
      curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
      chmod +x "$CACHE_MANAGER"
      echo "Cache manager ready: $CACHE_MANAGER (ref: ${CACHE_MANAGER_REF})"
      echo -e "\e[0Ksection_end:$(date +%s):cache_manager_setup\r\e[0K"

# Fetch extract-test-cache.sh script for test jobs
# This script handles cache extraction with marker files, PostgreSQL wait, and proper error handling.
# Usage:
#   before_script:
#     - !reference [.fetch_extract_test_cache, before_script]
#     - $EXTRACT_TEST_CACHE "${APP_SYNC_CACHE_TYPE}" "${APP_CACHE_KEY}" "${HAF_DATA_DIRECTORY}"
#
# Environment variables (optional):
#   POSTGRES_HOST       - If set, waits for PostgreSQL readiness
#   SKIP_POSTGRES_WAIT  - Set to "true" to skip wait
#   EXTRACT_TIMEOUT     - Timeout in seconds (default: 300)
.fetch_extract_test_cache:
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):extract_cache_setup[collapsed=true]\r\e[0KSetting up extract-test-cache..."
      EXTRACT_TEST_CACHE="/tmp/extract-test-cache.sh"
      curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF}/scripts/extract-test-cache.sh" -o "$EXTRACT_TEST_CACHE"
      chmod +x "$EXTRACT_TEST_CACHE"
      echo "Extract script ready: $EXTRACT_TEST_CACHE (ref: ${CACHE_MANAGER_REF})"
      echo -e "\e[0Ksection_end:$(date +%s):extract_cache_setup\r\e[0K"

# =============================================================================
# CHANGE DETECTION TEMPLATE
# =============================================================================
# Simplified detect_changes template - analyzes changed files to determine
# if full HAF sync/build can be skipped. When only tests/docs change, we can
# use cached HAF data.
#
# Required variables to set in derived job:
#   HAF_APP_SKIP_PATTERNS: Regex patterns for files that don't need sync
#   HAF_COMMIT: HAF commit (for cache key lookup)
#
# Outputs (via dotenv artifact):
#   AUTO_SKIP_BUILD: "true" if build/sync can be skipped
#   AUTO_SKIP_SYNC: alias for AUTO_SKIP_BUILD (compatibility)
#   CACHE_COMMIT: HAF commit to use for cached data
#   AUTO_CACHE_HAF_COMMIT: alias for CACHE_COMMIT (compatibility)
#   AUTO_CACHE_COMMIT: alias for CACHE_COMMIT (compatibility)
#
# Note: This template handles the common case. For apps with custom cache
# discovery logic, you can override the script section.

.haf_app_detect_changes:
  extends: .job-defaults
  stage: detect
  image: alpine:latest
  variables:
    GIT_SUBMODULE_STRATEGY: none
  before_script:
    - apk add --no-cache git
  script:
    - |
      set -e

      # Initialize defaults
      CAN_SKIP_BUILD=false
      CACHE_COMMIT=""

      # Handle manual QUICK_TEST mode
      if [ "$QUICK_TEST" = "true" ]; then
        echo "=== Manual QUICK_TEST Mode ==="
        CAN_SKIP_BUILD=true
        CACHE_COMMIT="${QUICK_TEST_HAF_COMMIT}"
        if [ -z "$CACHE_COMMIT" ]; then
          echo "ERROR: QUICK_TEST_HAF_COMMIT must be set when QUICK_TEST=true"
          echo ""
          echo "Find available caches:"
          echo "  ssh hive-builder-10 'ls -lt /nfs/ci-cache/haf/*.tar | head -5'"
          exit 1
        fi
        echo "Using cached data from: $CACHE_COMMIT"
      else
        # Automatic detection mode
        echo "=== Detecting Changed Files ==="

        # Get changed files based on pipeline type
        if [ -n "${CI_MERGE_REQUEST_DIFF_BASE_SHA:-}" ]; then
          BASE_SHA="$CI_MERGE_REQUEST_DIFF_BASE_SHA"
          echo "MR pipeline: comparing against target branch"
        elif [ "${CI_PIPELINE_SOURCE:-}" = "push" ]; then
          # For protected branches (develop/master), compare against previous commit
          # For feature branches, compare against merge-base with develop
          if [ "$CI_COMMIT_REF_NAME" = "develop" ] || [ "$CI_COMMIT_REF_NAME" = "master" ]; then
            BASE_SHA="HEAD~1"
            echo "Push to protected branch: comparing against previous commit"
          else
            BASE_SHA=$(git merge-base HEAD origin/develop 2>/dev/null || echo "HEAD~1")
            echo "Push to feature branch: comparing against develop merge-base"
          fi
        else
          BASE_SHA=$(git merge-base HEAD origin/develop 2>/dev/null || echo "HEAD~1")
          echo "Other pipeline: comparing against develop"
        fi

        echo "Comparing $BASE_SHA to HEAD"
        CHANGED_FILES=$(git diff --name-only "$BASE_SHA" HEAD 2>/dev/null || git show --name-only --pretty=format: HEAD | grep -v '^$' || true)

        # Last resort: require full build
        if [ -z "$CHANGED_FILES" ]; then
          echo "WARNING: Could not determine changed files - requiring full build"
          CHANGED_FILES="unknown-changes"
        fi

        echo "Changed files:"
        # Use printf with process substitution to avoid SIGPIPE from head
        echo "$CHANGED_FILES" | head -50 || true
        FILE_COUNT=$(echo "$CHANGED_FILES" | wc -l)
        if [ "$FILE_COUNT" -gt 50 ]; then
          echo "... (showing 50 of $FILE_COUNT files)"
        fi

        NEEDS_BUILD=$(echo "$CHANGED_FILES" | grep -vE "${HAF_APP_SKIP_PATTERNS}" || true)

        if [ -z "$NEEDS_BUILD" ]; then
          echo ""
          echo "=== Can skip data prep (only tests/docs changed) ==="
          CAN_SKIP_BUILD=true
          # Don't set CACHE_COMMIT here - smart_cache_lookup will find an available cache
          # The app's AUTO_CACHE_HAF_COMMIT variable (if set) will be used
        else
          echo ""
          echo "=== Files requiring full build: ==="
          echo "$NEEDS_BUILD" | head -50 || true
          BUILD_COUNT=$(echo "$NEEDS_BUILD" | wc -l)
          if [ "$BUILD_COUNT" -gt 50 ]; then
            echo "... (showing 50 of $BUILD_COUNT files)"
          fi
        fi
      fi

      # Write dotenv artifact
      # AUTO_SKIP_SYNC tells downstream jobs they can use cached data
      # AUTO_CACHE_HAF_COMMIT is only set for QUICK_TEST mode (explicit cache selection)
      # For auto-skip mode, smart_cache_lookup will find an available cache
      echo "AUTO_SKIP_BUILD=${CAN_SKIP_BUILD}" > detect_changes.env
      echo "AUTO_SKIP_SYNC=${CAN_SKIP_BUILD}" >> detect_changes.env
      if [ -n "$CACHE_COMMIT" ]; then
        echo "CACHE_COMMIT=${CACHE_COMMIT}" >> detect_changes.env
        echo "AUTO_CACHE_HAF_COMMIT=${CACHE_COMMIT}" >> detect_changes.env
        echo "AUTO_CACHE_COMMIT=${CACHE_COMMIT}" >> detect_changes.env
      fi

      echo ""
      echo "=== Detection Results ==="
      echo "AUTO_SKIP_BUILD/SYNC: $CAN_SKIP_BUILD"
      echo "CACHE_COMMIT: ${CACHE_COMMIT:-<not set, will use app default>}"

  artifacts:
    reports:
      dotenv: detect_changes.env
    expire_in: 1 day
  rules:
    # Never run for tags (always full build for releases)
    - if: $CI_COMMIT_TAG
      when: never
    - when: on_success

# =============================================================================
# HAF COMMIT VALIDATION TEMPLATE
# =============================================================================
# Ensures HAF_COMMIT variable matches include ref and submodule commit.
# This prevents subtle bugs from version mismatches.

.haf_commit_validation:
  extends: .job-defaults
  stage: build
  image: alpine:latest
  variables:
    # HAF_COMMIT should come from global variables or be set in derived job
    # Do NOT set a default here as it would override global variables
    HAF_INCLUDE_REF: ""      # Should match the 'ref' in your HAF include
    HAF_SUBMODULE_PATH: "haf"  # Path to HAF submodule
  before_script:
    - apk add --no-cache git
  script:
    - |
      set -euo pipefail

      echo "=== HAF Commit Validation ==="
      echo "HAF_COMMIT variable: ${HAF_COMMIT}"
      echo "HAF_INCLUDE_REF: ${HAF_INCLUDE_REF:-not set}"
      echo "HAF_SUBMODULE_PATH: ${HAF_SUBMODULE_PATH}"

      ERRORS=0

      # Check submodule commit matches
      if [[ -d "${HAF_SUBMODULE_PATH}/.git" ]] || [[ -f "${HAF_SUBMODULE_PATH}/.git" ]]; then
        SUBMODULE_COMMIT=$(git -C "${HAF_SUBMODULE_PATH}" rev-parse HEAD 2>/dev/null || echo "")
        echo "Submodule commit: ${SUBMODULE_COMMIT}"

        if [[ -n "$SUBMODULE_COMMIT" && "$SUBMODULE_COMMIT" != "$HAF_COMMIT" ]]; then
          echo "ERROR: HAF_COMMIT ($HAF_COMMIT) does not match submodule ($SUBMODULE_COMMIT)"
          ERRORS=$((ERRORS + 1))
        fi
      else
        echo "WARNING: HAF submodule not found at ${HAF_SUBMODULE_PATH}"
      fi

      # Check include ref matches (if provided)
      if [[ -n "${HAF_INCLUDE_REF:-}" && "$HAF_INCLUDE_REF" != "$HAF_COMMIT" ]]; then
        echo "ERROR: HAF_COMMIT ($HAF_COMMIT) does not match include ref ($HAF_INCLUDE_REF)"
        ERRORS=$((ERRORS + 1))
      fi

      if [[ $ERRORS -gt 0 ]]; then
        echo ""
        echo "VALIDATION FAILED: $ERRORS errors found"
        echo "Please ensure HAF_COMMIT, include ref, and submodule are all in sync"
        exit 1
      fi

      echo ""
      echo "VALIDATION PASSED"
  tags:
    - public-runner-docker

# =============================================================================
# DOCKER-IN-DOCKER TEST BASE TEMPLATE
# =============================================================================
# Base template for running tests with Docker Compose in a DinD environment.
# Provides proper cache extraction and service orchestration.
#
# Required variables:
#   HAF_APP_CACHE_TYPE: Cache type to extract
#   HAF_APP_CACHE_KEY: Cache key (usually from detect_changes or sync job)
#
# Optional:
#   COMPOSE_FILE: Path to docker-compose file (default: docker/docker-compose-test.yml)
#   COMPOSE_PROJECT_NAME: Docker Compose project name
#   HAF_DATA_DIRECTORY: Where to extract cache data

.haf_app_dind_test_base:
  extends: .docker_image_builder_job_template
  stage: test
  timeout: 30 minutes
  variables:
    FF_NETWORK_PER_BUILD: "true"
    DOCKER_TLS_CERTDIR: ""
    DOCKER_HOST: "tcp://docker:2375"

    # Cache configuration
    HAF_APP_CACHE_TYPE: ""    # Must be set
    HAF_APP_CACHE_KEY: ""     # Must be set (from detect_changes or sync)
    HAF_DATA_DIRECTORY: "${CI_PROJECT_DIR}/.haf-data"

    # Docker Compose configuration
    COMPOSE_FILE: "docker/docker-compose-test.yml"
    COMPOSE_PROJECT_NAME: "haf-test-${CI_JOB_ID}"

  before_script:
    - !reference [.docker_image_builder_job_template, before_script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):cache_setup[collapsed=true]\r\e[0KSetting up HAF cache..."

      # Fetch cache-manager
      mkdir -p "$(dirname "$CACHE_MANAGER")"
      curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
      chmod +x "$CACHE_MANAGER"

      # Extract cache
      echo "Extracting cache: ${HAF_APP_CACHE_TYPE}/${HAF_APP_CACHE_KEY}"
      mkdir -p "${HAF_DATA_DIRECTORY}"
      "$CACHE_MANAGER" get "${HAF_APP_CACHE_TYPE}" "${HAF_APP_CACHE_KEY}" "${HAF_DATA_DIRECTORY}"

      echo -e "\e[0Ksection_end:$(date +%s):cache_setup\r\e[0K"
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose_start[collapsed=true]\r\e[0KStarting Docker Compose services..."

      # Start services
      if [[ -f "${COMPOSE_FILE}" ]]; then
        docker compose -f "${COMPOSE_FILE}" -p "${COMPOSE_PROJECT_NAME}" up -d --quiet-pull

        # Wait for services to be healthy
        echo "Waiting for services to be ready..."
        WAIT_START=$(date +%s)
        WAIT_TIMEOUT=${HAF_EXTRACT_TIMEOUT:-300}

        while true; do
          UNHEALTHY=$(docker compose -f "${COMPOSE_FILE}" -p "${COMPOSE_PROJECT_NAME}" ps --format json 2>/dev/null | jq -r 'select(.Health != "healthy" and .Health != "" and .State == "running") | .Name' | wc -l || echo "0")

          if [[ "$UNHEALTHY" == "0" ]]; then
            echo "All services healthy"
            break
          fi

          ELAPSED=$(($(date +%s) - WAIT_START))
          if [[ $ELAPSED -ge $WAIT_TIMEOUT ]]; then
            echo "WARNING: Timeout waiting for services (${WAIT_TIMEOUT}s)"
            docker compose -f "${COMPOSE_FILE}" -p "${COMPOSE_PROJECT_NAME}" ps
            break
          fi

          echo "Waiting for $UNHEALTHY services... (${ELAPSED}s)"
          sleep 5
        done
      else
        echo "No compose file found at ${COMPOSE_FILE}, skipping service startup"
      fi

      echo -e "\e[0Ksection_end:$(date +%s):compose_start\r\e[0K"

  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose_cleanup\r\e[0KCleaning up Docker Compose..."

      if [[ -f "${COMPOSE_FILE:-docker/docker-compose-test.yml}" ]]; then
        # Capture logs before cleanup
        docker compose -f "${COMPOSE_FILE:-docker/docker-compose-test.yml}" -p "${COMPOSE_PROJECT_NAME:-haf-test}" logs --no-color > container-logs.txt 2>&1 || true

        # Cleanup
        docker compose -f "${COMPOSE_FILE:-docker/docker-compose-test.yml}" -p "${COMPOSE_PROJECT_NAME:-haf-test}" down -v --remove-orphans || true
      fi

      echo -e "\e[0Ksection_end:$(date +%s):compose_cleanup\r\e[0K"

  artifacts:
    when: always
    paths:
      - container-logs.txt
    expire_in: 1 week

  tags:
    - data-cache-storage
    - fast

# =============================================================================
# TAVERN TEST HELPERS
# =============================================================================
# Composable templates for running Tavern API tests.
# Apps can mix these with their own DinD/cache setup.

# Variables for tavern tests (set these in your job)
# Note: pytest 8.0+ and tavern 2.10+ required for Python 3.12+ compatibility
.tavern_test_variables:
  variables:
    PYTEST_WORKERS: "4"
    PYTEST_VERSION: "8.0.0"
    TAVERN_VERSION: "2.10.0"
    TAVERN_DIR: "tests/tavern"
    JUNIT_REPORT: "tests/tavern/report.xml"

# Script to install tavern dependencies in a venv
# Use with: !reference [.tavern_install_deps, script]
# Note: pytest 8.0+ required for Python 3.12+ (ast.Str removed in 3.12)
#       tavern 2.10+ required for pytest 8 compatibility
.tavern_install_deps:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tavern_deps[collapsed=true]\r\e[0KInstalling Tavern dependencies..."
      python3 -m venv venv
      . venv/bin/activate
      pip install --quiet \
        "pytest>=${PYTEST_VERSION:-8.0.0}" \
        "tavern>=${TAVERN_VERSION:-2.10.0}" \
        "pytest-xdist>=3.2.0" \
        "pyyaml>=6.0" \
        "requests>=2.28.0" \
        "deepdiff>=6.0" \
        "prettytable>=3.0"
      echo -e "\e[0Ksection_end:$(date +%s):tavern_deps\r\e[0K"

# Script to run tavern tests with pytest
# Requires: venv activated, TAVERN_DIR set, JUNIT_REPORT set
# Use with: !reference [.tavern_run_tests, script]
.tavern_run_tests:
  script:
    - |
      echo "=== Running Tavern API Tests ==="
      . venv/bin/activate
      cd "${TAVERN_DIR}"
      pytest -n "${PYTEST_WORKERS:-4}" --junitxml "${JUNIT_REPORT}" . || exit $?

# Artifacts configuration for tavern tests
# Use with: artifacts: !reference [.tavern_test_artifacts, artifacts]
.tavern_test_artifacts:
  artifacts:
    when: always
    paths:
      - "**/*.out.json"
      - docker/container-logs.tar.gz
      - docker/docker-compose-logs.txt
    reports:
      junit: "${JUNIT_REPORT}"
    expire_in: 1 week

# Combined tavern test job template (for simple cases)
# Extends .docker_image_builder_job_template, provides test execution
# Apps must provide their own before_script for cache/compose setup
.haf_app_tavern_tests:
  extends:
    - .docker_image_builder_job_template
    - .tavern_test_variables
  stage: test
  timeout: 30 minutes
  script:
    - !reference [.tavern_install_deps, script]
    - !reference [.tavern_run_tests, script]
  artifacts: !reference [.tavern_test_artifacts, artifacts]

# Combined DinD + Tavern test template
# This template provides a complete DinD test environment with Tavern setup:
# - Cache extraction and docker-compose startup
# - Service readiness checks (PostgreSQL + PostgREST)
# - Tavern dependency installation
# - Automatic cleanup
#
# Required variables from app:
#   APP_SYNC_CACHE_TYPE: Cache type (e.g., "haf_btracker_sync")
#   APP_CACHE_KEY: Cache key from sync job
#   HAF_IMAGE_NAME: HAF Docker image
#   HAF_APP_ROLE_PREFIX: Role prefix for PostgREST (e.g., "btracker")
#
# Apps can add custom before_script steps after extending, e.g.:
#   before_script:
#     - !reference [.haf_app_dind_tavern_tests, before_script]
#     - pip install -e ./tests_api  # App-specific module
.haf_app_dind_tavern_tests:
  extends:
    - .haf_app_dind_test_variables
    - .tavern_test_variables
  stage: test
  timeout: 30 minutes
  before_script:
    - !reference [.haf_app_dind_extract_cache, script]
    - !reference [.haf_app_dind_compose_startup, script]
    - !reference [.haf_app_dind_wait_for_services, script]
    - !reference [.tavern_install_deps, script]
  after_script: !reference [.haf_app_dind_compose_teardown, after_script]
  artifacts: !reference [.tavern_test_artifacts, artifacts]
  tags:
    - data-cache-storage
    - fast

# =============================================================================
# QUICK TEST MODE HELPERS
# =============================================================================
# Templates for handling QUICK_TEST mode (manual cache selection)

.quick_test_variables:
  variables:
    # Set these to use quick test mode
    QUICK_TEST: ""                    # Set to "true" to enable
    QUICK_TEST_HAF_COMMIT: ""         # Required if QUICK_TEST=true
    QUICK_TEST_APP_COMMIT: ""         # Optional, defaults to current commit

# Rule to skip jobs when QUICK_TEST is enabled
.skip_on_quick_test:
  rules:
    - if: $QUICK_TEST == "true"
      when: never
    - when: on_success

# Rule to skip jobs when AUTO_SKIP_SYNC is true (from detect_changes)
.skip_on_auto_skip:
  rules:
    - if: $AUTO_SKIP_SYNC == "true"
      when: never
    - when: on_success

# Combined rule for both skip conditions
.skip_on_cached_data:
  rules:
    - if: $QUICK_TEST == "true"
      when: never
    - if: $AUTO_SKIP_SYNC == "true"
      when: never
    - when: on_success

# =============================================================================
# EARLY EXIT TEMPLATES
# =============================================================================
# These templates allow jobs to RUN but EXIT EARLY when no work is needed.
# Use these when you want the job to appear as "passed" rather than "skipped".
#
# Difference from .skip_on_* rules:
#   - .skip_on_* rules: Job never runs (shows as "skipped" in pipeline)
#   - .early_exit_*: Job runs but exits immediately (shows as "passed")
#
# Use early exit when:
#   - You need the job to exist for dependency chains
#   - You want explicit "nothing to do" feedback in logs
#   - The job has artifacts that downstream jobs expect (even if empty)

# Early exit script for docs/tests-only changes
# Use with !reference in your script section:
#   script:
#     - !reference [.haf_app_early_exit_on_skip, script]
#     - # Your actual work here...
.haf_app_early_exit_on_skip:
  script:
    - |
      if [[ "${AUTO_SKIP_SYNC:-false}" == "true" ]] || [[ "${AUTO_SKIP_BUILD:-false}" == "true" ]]; then
        echo "=== Early Exit: No work needed ==="
        echo "Only documentation, tests, or non-critical files changed."
        echo "Exiting with success."
        exit 0
      fi

# Early exit for QUICK_TEST mode
.haf_app_early_exit_on_quick_test:
  script:
    - |
      if [[ "${QUICK_TEST:-false}" == "true" ]]; then
        echo "=== Early Exit: QUICK_TEST mode ==="
        echo "Using pre-cached data, skipping this job."
        exit 0
      fi

# Combined early exit (either condition)
.haf_app_early_exit_on_cached:
  script:
    - |
      if [[ "${QUICK_TEST:-false}" == "true" ]]; then
        echo "=== Early Exit: QUICK_TEST mode ==="
        echo "Using pre-cached data, skipping this job."
        exit 0
      fi
      if [[ "${AUTO_SKIP_SYNC:-false}" == "true" ]] || [[ "${AUTO_SKIP_BUILD:-false}" == "true" ]]; then
        echo "=== Early Exit: No work needed ==="
        echo "Only documentation, tests, or non-critical files changed."
        exit 0
      fi

# =============================================================================
# SKIP PATTERN PRESETS
# =============================================================================
# Common skip pattern configurations for different app types.
# Override HAF_APP_SKIP_PATTERNS in your detect_changes job.

# Standard HAF app patterns (tests, docs, markdown, CI config)
# Use: extends: .haf_app_skip_patterns_standard
.haf_app_skip_patterns_standard:
  variables:
    HAF_APP_SKIP_PATTERNS: '^tests/|^docs/|\.md$|^README|^CHANGELOG|^LICENSE|^CLAUDE|^\.gitlab-ci'

# HAF app with GUI (adds gui/ directory to skip list)
# Use: extends: .haf_app_skip_patterns_with_gui
.haf_app_skip_patterns_with_gui:
  variables:
    HAF_APP_SKIP_PATTERNS: '^tests/|^docs/|^gui/|\.md$|^README|^CHANGELOG|^LICENSE|^CLAUDE|^\.gitlab-ci'

# =============================================================================
# SYNC JOB COMPONENTS
# =============================================================================
# Composable building blocks for HAF app sync jobs.
# Use !reference to compose your sync job from these components.
#
# Typical sync job structure:
#   extends: .docker_image_builder_job_template
#   before_script:
#     - !reference [.haf_app_sync_setup, script]
#     - !reference [.haf_app_fetch_haf_cache, script]
#   script:
#     - # Your app-specific sync logic
#     - !reference [.haf_app_sync_shutdown, script]
#     - !reference [.haf_app_sync_save_cache, script]
#   after_script: !reference [.haf_app_sync_cleanup, after_script]
#   artifacts: !reference [.haf_app_sync_artifacts, artifacts]

# Common sync job variables
.haf_app_sync_variables:
  variables:
    # Directories (using CI_JOB_ID for isolation)
    DATADIR: "${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir"
    SHM_DIR: "${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir"
    HAF_DATA_DIRECTORY: "${DATADIR}"
    HAF_SHM_DIRECTORY: "${SHM_DIR}"
    # Docker compose options (override per-app)
    COMPOSE_OPTIONS_STRING: "--env-file ci.env --file docker-compose.yml --ansi never"
    # Timeout for graceful PostgreSQL shutdown
    COMPOSE_DOWN_TIMEOUT: "60"
    # Postgres user in HAF container (UID:GID)
    POSTGRES_UID: "105"
    POSTGRES_GID: "109"

# Docker login and git safe.directory setup
# Use: !reference [.haf_app_sync_setup, script]
.haf_app_sync_setup:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
      git config --global --add safe.directory "$CI_PROJECT_DIR"
      git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
      echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"

# Fetch HAF replay data from local cache or NFS (simple version)
# Requires: HAF_COMMIT, DATA_CACHE_HAF_PREFIX variables
# Use: !reference [.haf_app_fetch_haf_cache, script]
.haf_app_fetch_haf_cache:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):haf_cache[collapsed=true]\r\e[0KFetching HAF replay data..."
      LOCAL_HAF_CACHE="${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}"
      if [[ -d "${LOCAL_HAF_CACHE}/datadir" ]]; then
        echo "Local HAF cache found at ${LOCAL_HAF_CACHE}"
      else
        echo "Local HAF cache not found, fetching from NFS..."
        # Fetch cache-manager if not already present
        if [[ ! -x "$CACHE_MANAGER" ]]; then
          curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
          chmod +x "$CACHE_MANAGER"
        fi
        if "$CACHE_MANAGER" get haf "${HAF_COMMIT}" "${LOCAL_HAF_CACHE}"; then
          echo "Fetched HAF replay data from NFS cache"
        else
          echo "ERROR: Failed to fetch HAF replay data from NFS cache"
          exit 1
        fi
      fi
      echo -e "\e[0Ksection_end:$(date +%s):haf_cache\r\e[0K"

# Advanced cache lookup with QUICK_TEST support
# Checks for existing app sync cache first (avoiding re-sync), then falls back to HAF cache.
# NOTE: This template does NOT search for caches from different app versions.
# The cache key includes both HAF commit and app commit to ensure schema compatibility.
#
# Requires variables:
#   HAF_COMMIT: Current HAF submodule commit
#   APP_SYNC_CACHE_TYPE: App-specific cache type (e.g., "haf_btracker_sync")
#   DATA_CACHE_HAF_PREFIX: Local cache directory prefix
#   DATA_CACHE_NFS_PREFIX: NFS cache directory prefix
#
# Optional variables (from detect_changes job):
#   QUICK_TEST: Set to "true" to use QUICK_TEST_HAF_COMMIT
#   QUICK_TEST_HAF_COMMIT: HAF commit to use in quick test mode
#   AUTO_CACHE_HAF_COMMIT: HAF commit from detect_changes (used with QUICK_TEST)
#
# Outputs (written to /tmp for use in script section):
#   /tmp/cache_hit: "true" if app sync cache was found (skip sync)
#   /tmp/effective_cache_key: The cache key to use for saving
#   /tmp/effective_haf_commit: The HAF commit being used
#
# Use: !reference [.haf_app_smart_cache_lookup, script]
.haf_app_smart_cache_lookup:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):cache_lookup[collapsed=true]\r\e[0KSmart cache lookup..."

      # Determine effective HAF commit based on mode
      # Default: use HAF_COMMIT from find_haf_image (dynamic, always current)
      EFFECTIVE_HAF_COMMIT="${HAF_COMMIT}"
      if [[ "${QUICK_TEST:-false}" == "true" ]]; then
        echo "=== QUICK_TEST Mode (manual) ==="
        if [[ -z "${QUICK_TEST_HAF_COMMIT}" ]]; then
          echo "ERROR: QUICK_TEST=true but QUICK_TEST_HAF_COMMIT not set"
          echo ""
          echo "Find available caches:"
          echo "  ssh hive-builder-10 'ls -lt /nfs/ci-cache/haf/*.tar | head -5'"
          exit 1
        fi
        EFFECTIVE_HAF_COMMIT="${QUICK_TEST_HAF_COMMIT}"
        echo "Using cached HAF data from: ${EFFECTIVE_HAF_COMMIT}"
      elif [[ "${AUTO_SKIP_SYNC:-false}" == "true" ]]; then
        echo "=== AUTO_SKIP_SYNC Mode (only tests/docs changed) ==="
        # Use HAF_COMMIT from find_haf_image - if no cache exists, sync runs normally
        echo "Using HAF commit from find_haf_image: ${EFFECTIVE_HAF_COMMIT}"
      fi

      # Build cache key and job data directory
      # Cache-manager extracts directly to job directory - no copy needed
      # DATADIR and SHM_DIR are children of JOB_DATA_DIR
      EFFECTIVE_CACHE_KEY="${EFFECTIVE_HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}"
      JOB_DATA_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"

      # Fetch cache-manager
      if [[ ! -x "$CACHE_MANAGER" ]]; then
        curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
        chmod +x "$CACHE_MANAGER"
      fi

      CACHE_HIT="false"

      # Check for existing app sync cache - extract directly to job directory
      echo "Checking for app sync cache: ${APP_SYNC_CACHE_TYPE}/${EFFECTIVE_CACHE_KEY}"
      if "$CACHE_MANAGER" get "${APP_SYNC_CACHE_TYPE}" "${EFFECTIVE_CACHE_KEY}" "${JOB_DATA_DIR}" 2>/dev/null; then
        echo "App sync cache found - skipping sync"
        CACHE_HIT="true"
      else
        echo "App sync cache not found (key: ${EFFECTIVE_CACHE_KEY}), will use HAF-only cache and run sync"
      fi

      # Fall back to HAF-only cache if no app cache found
      if [[ "$CACHE_HIT" != "true" ]]; then
        echo "Fetching HAF cache..."
        if "$CACHE_MANAGER" get haf "${EFFECTIVE_HAF_COMMIT}" "${JOB_DATA_DIR}"; then
          echo "HAF replay data ready"
        else
          echo "ERROR: Failed to fetch HAF replay data"
          exit 1
        fi
      fi

      # DATA_SOURCE points to where data was extracted (for compatibility)
      export DATA_SOURCE="${JOB_DATA_DIR}"

      # Relocate shm_dir from inside datadir to be a sibling (for old caches)
      # New caches store them as siblings, but old caches had shm_dir inside datadir
      if [[ -d "${JOB_DATA_DIR}/datadir/shm_dir" && ! -d "${JOB_DATA_DIR}/shm_dir" ]]; then
        echo "Moving shm_dir from datadir/ to be a sibling..."
        sudo mv "${JOB_DATA_DIR}/datadir/shm_dir" "${JOB_DATA_DIR}/shm_dir"
      fi

      # Export state for script section
      echo "$CACHE_HIT" > /tmp/cache_hit
      echo "$EFFECTIVE_CACHE_KEY" > /tmp/effective_cache_key
      echo "$EFFECTIVE_HAF_COMMIT" > /tmp/effective_haf_commit

      echo "Cache lookup result: CACHE_HIT=$CACHE_HIT, KEY=$EFFECTIVE_CACHE_KEY"
      echo -e "\e[0Ksection_end:$(date +%s):cache_lookup\r\e[0K"

# Conditional save cache - only saves if CACHE_HIT is false
# Use this instead of .haf_app_sync_save_cache when using smart cache lookup
# Requires: APP_SYNC_CACHE_TYPE, DATADIR, SHM_DIR, /tmp/cache_hit, /tmp/effective_cache_key
# Use: !reference [.haf_app_sync_save_cache_conditional, script]
.haf_app_sync_save_cache_conditional:
  script:
    - |
      CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")
      EFFECTIVE_CACHE_KEY=$(cat /tmp/effective_cache_key 2>/dev/null || echo "${APP_CACHE_KEY}")

      if [[ "$CACHE_HIT" == "true" ]]; then
        echo "Cache hit - skipping cache save (data already in cache)"
      else
        echo -e "\e[0Ksection_start:$(date +%s):save_cache[collapsed=true]\r\e[0KSaving sync data to cache..."

        # Save to local cache - keep datadir and shm_dir as siblings (matching HAF cache structure)
        LOCAL_APP_CACHE="${DATA_CACHE_HAF_PREFIX}_${APP_SYNC_CACHE_TYPE}_${EFFECTIVE_CACHE_KEY}"
        mkdir -p "${LOCAL_APP_CACHE}"
        sudo cp -a "${DATADIR}" "${LOCAL_APP_CACHE}/datadir"
        sudo cp -a "${SHM_DIR}" "${LOCAL_APP_CACHE}/shm_dir"

        # Remove empty blockchain dir to trigger symlink on test runners
        if [[ -d "${LOCAL_APP_CACHE}/datadir/blockchain" ]] && [[ -z "$(ls -A "${LOCAL_APP_CACHE}/datadir/blockchain" 2>/dev/null)" ]]; then
          echo "Removing empty blockchain directory from local cache"
          rmdir "${LOCAL_APP_CACHE}/datadir/blockchain" 2>/dev/null || true
        fi

        ls -lah "${LOCAL_APP_CACHE}"
        ls -lah "${LOCAL_APP_CACHE}/datadir" || true

        # Push to NFS cache
        if [[ ! -x "$CACHE_MANAGER" ]]; then
          curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
          chmod +x "$CACHE_MANAGER"
        fi

        echo "Pushing sync data to NFS: ${APP_SYNC_CACHE_TYPE}/${EFFECTIVE_CACHE_KEY}"
        if CACHE_HANDLING=haf "$CACHE_MANAGER" put "${APP_SYNC_CACHE_TYPE}" "${EFFECTIVE_CACHE_KEY}" "${LOCAL_APP_CACHE}"; then
          echo "Successfully pushed cache to NFS"
        else
          echo "WARNING: Failed to push to NFS cache"
        fi
        echo -e "\e[0Ksection_end:$(date +%s):save_cache\r\e[0K"
      fi

# Copy datadir from HAF cache and fix ownership/permissions
# Requires: HAF submodule with scripts/copy_datadir.sh, DATADIR, POSTGRES_UID/GID
# Use: !reference [.haf_app_copy_datadir, script]
.haf_app_copy_datadir:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):copy_datadir[collapsed=true]\r\e[0KCopying HAF datadir..."
      "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

      # Fix pgdata ownership and permissions for HAF container
      if [[ -d "${DATADIR}/haf_db_store" ]]; then
        echo "Fixing haf_db_store ownership to UID ${POSTGRES_UID}:${POSTGRES_GID}"
        sudo chown -R "${POSTGRES_UID}:${POSTGRES_GID}" "${DATADIR}/haf_db_store"
        sudo chown -R "${POSTGRES_UID}:${POSTGRES_GID}" "${DATADIR}/haf_postgresql_conf.d" 2>/dev/null || true
        # Fix pgdata permissions - PostgreSQL requires 700 or 750
        if [[ -d "${DATADIR}/haf_db_store/pgdata" ]]; then
          echo "Fixing pgdata permissions to 700"
          sudo chmod 700 "${DATADIR}/haf_db_store/pgdata"
        fi
      fi
      echo -e "\e[0Ksection_end:$(date +%s):copy_datadir\r\e[0K"

# Copy block_log to docker directory
# Requires: BLOCK_LOG_SOURCE_DIR_5M variable
# Use: !reference [.haf_app_copy_blockchain, script]
.haf_app_copy_blockchain:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):blockchain[collapsed=true]\r\e[0KCopying blockchain files..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp "${BLOCK_LOG_SOURCE_DIR_5M}/block_log" "${CI_PROJECT_DIR}/docker/blockchain/block_log"
      cp "${BLOCK_LOG_SOURCE_DIR_5M}/block_log.artifacts" "${CI_PROJECT_DIR}/docker/blockchain/block_log.artifacts"
      chmod a+w docker/blockchain/block_log
      echo -e "\e[0Ksection_end:$(date +%s):blockchain\r\e[0K"

# Shutdown containers: checkpoint PostgreSQL, collect logs, compose down
# Requires: COMPOSE_OPTIONS_STRING, COMPOSE_DOWN_TIMEOUT variables
# Use: !reference [.haf_app_sync_shutdown, script]
.haf_app_sync_shutdown:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):shutdown[collapsed=true]\r\e[0KStopping containers..."
      pushd docker
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING

      # Force PostgreSQL checkpoint before shutdown to ensure all data is written to disk
      echo "Forcing PostgreSQL checkpoint..."
      docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" || true

      # Collect logs before stopping
      docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log || true
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > backend-setup.log || true
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log || true
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > backend-postgrest.log || true

      # Use longer timeout for graceful PostgreSQL shutdown
      docker compose "${COMPOSE_OPTIONS[@]}" down --volumes --timeout "${COMPOSE_DOWN_TIMEOUT:-60}"
      popd

      # Archive logs
      tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log
      echo -e "\e[0Ksection_end:$(date +%s):shutdown\r\e[0K"

# Save sync data to local cache and push to NFS
# Requires: APP_SYNC_CACHE_TYPE, APP_CACHE_KEY, DATADIR, SHM_DIR
# Use: !reference [.haf_app_sync_save_cache, script]
.haf_app_sync_save_cache:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):save_cache[collapsed=true]\r\e[0KSaving sync data to cache..."

      # Save to local cache - keep datadir and shm_dir as siblings (matching HAF cache structure)
      LOCAL_APP_CACHE="${DATA_CACHE_HAF_PREFIX}_${APP_SYNC_CACHE_TYPE}_${APP_CACHE_KEY}"
      mkdir -p "${LOCAL_APP_CACHE}"
      sudo cp -a "${DATADIR}" "${LOCAL_APP_CACHE}/datadir"
      sudo cp -a "${SHM_DIR}" "${LOCAL_APP_CACHE}/shm_dir"

      # Copy blockchain files to cache
      mkdir -p "${LOCAL_APP_CACHE}/datadir/blockchain"
      # Remove any existing symlinks before copying (they may point to read-only locations)
      sudo rm -f "${LOCAL_APP_CACHE}/datadir/blockchain/block_log" "${LOCAL_APP_CACHE}/datadir/blockchain/block_log.artifacts"
      sudo cp -a "${CI_PROJECT_DIR}/docker/blockchain/block_log" "${LOCAL_APP_CACHE}/datadir/blockchain/block_log"
      sudo cp -a "${CI_PROJECT_DIR}/docker/blockchain/block_log.artifacts" "${LOCAL_APP_CACHE}/datadir/blockchain/block_log.artifacts"

      ls -lah "${LOCAL_APP_CACHE}"
      ls -lah "${LOCAL_APP_CACHE}/datadir" || true

      # Push to NFS cache
      if [[ ! -x "$CACHE_MANAGER" ]]; then
        curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
        chmod +x "$CACHE_MANAGER"
      fi

      echo "Pushing sync data to NFS: ${APP_SYNC_CACHE_TYPE}/${APP_CACHE_KEY}"
      if CACHE_HANDLING=haf "$CACHE_MANAGER" put "${APP_SYNC_CACHE_TYPE}" "${APP_CACHE_KEY}" "${LOCAL_APP_CACHE}"; then
        echo "Successfully pushed cache to NFS"
      else
        echo "WARNING: Failed to push to NFS cache"
      fi
      echo -e "\e[0Ksection_end:$(date +%s):save_cache\r\e[0K"

# Cleanup job directory in after_script
# Use: after_script: !reference [.haf_app_sync_cleanup, after_script]
.haf_app_sync_cleanup:
  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up..."
      sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID} 2>/dev/null || true
      echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"

# Standard artifacts for sync jobs
# Use: artifacts: !reference [.haf_app_sync_artifacts, artifacts]
.haf_app_sync_artifacts:
  artifacts:
    paths:
      - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always

# =============================================================================
# CLEANUP TEMPLATES
# =============================================================================
# Manual cleanup jobs for cache data

.haf_app_cache_cleanup:
  extends: .job-defaults
  stage: cleanup
  image: alpine:latest
  variables:
    HAF_APP_CACHE_TYPE: ""    # Must be set
    CLEANUP_PATTERN: ""       # Optional glob pattern
  before_script:
    - apk add --no-cache bash
  script:
    - |
      set -euo pipefail

      echo "=== Cache Cleanup ==="
      echo "Cache type: ${HAF_APP_CACHE_TYPE}"
      echo "Local prefix: ${DATA_CACHE_LOCAL_PREFIX}"

      # Build pattern
      if [[ -n "${CLEANUP_PATTERN}" ]]; then
        PATTERN="${DATA_CACHE_LOCAL_PREFIX}/${CLEANUP_PATTERN}"
      else
        PATTERN="${DATA_CACHE_LOCAL_PREFIX}/${HAF_APP_CACHE_TYPE}_*"
      fi

      echo "Cleanup pattern: $PATTERN"
      echo ""
      echo "Files to remove:"
      ls -la $PATTERN 2>/dev/null || echo "(none found)"

      echo ""
      echo "Removing..."
      rm -rf $PATTERN || true
      echo "Done"
  when: manual
  allow_failure: true
  tags:
    - data-cache-storage

# =============================================================================
# LINT TEMPLATES
# =============================================================================
# Reusable linting job templates for HAF applications.
# These templates provide consistent linting across all HAF apps.
#
# Usage:
#   lint_bash:
#     extends: .haf_app_lint_bash
#     variables:
#       LINT_SCRIPTS_DIR: "scripts"  # Optional, defaults to "scripts"
#
#   lint_sql:
#     extends: .haf_app_lint_sql
#     variables:
#       SQLFLUFF_CONFIG: ".sqlfluff"  # Optional

# Bash script linting with shellcheck
# Generates checkstyle XML report for GitLab code quality integration
.haf_app_lint_bash:
  extends: .job-defaults
  stage: build
  image: koalaman/shellcheck-alpine:stable
  variables:
    LINT_SCRIPTS_DIR: "scripts"
    # Space-separated list of paths to exclude (e.g., "haf .git vendor")
    LINT_EXCLUDE_PATHS: ""
    GIT_SUBMODULE_STRATEGY: none
  before_script:
    - apk add --no-cache findutils xmlstarlet
  script:
    - |
      echo "=== Linting Bash Scripts in ${LINT_SCRIPTS_DIR}/ ==="

      # Build find exclusion arguments
      FIND_EXCLUDES=""
      for EXCLUDE in ${LINT_EXCLUDE_PATHS}; do
        FIND_EXCLUDES="${FIND_EXCLUDES} -path ./${EXCLUDE} -prune -o"
      done
      # Always exclude .git
      FIND_EXCLUDES="-path ./.git -prune -o ${FIND_EXCLUDES}"

      # Find and lint all shell scripts (excluding specified paths)
      SCRIPTS=$(eval "find ${LINT_SCRIPTS_DIR}/ ${FIND_EXCLUDES} -name '*.sh' -type f -print" 2>/dev/null || true)

      if [[ -z "$SCRIPTS" ]]; then
        echo "No .sh files found in ${LINT_SCRIPTS_DIR}/"
        exit 0
      fi

      echo "Found $(echo "$SCRIPTS" | wc -l) script(s) to lint"
      echo "$SCRIPTS" | head -20

      # Run shellcheck with checkstyle output for CI integration
      # Use xargs to handle file list properly
      echo "$SCRIPTS" | xargs shellcheck -f checkstyle 2>&1 | tee shellcheck.xml || true

      # Convert to HTML report if xmlstarlet available
      if command -v xmlstarlet >/dev/null 2>&1; then
        xmlstarlet tr /usr/share/checkstyle/checkstyle-noframes.xsl shellcheck.xml > shellcheck.html 2>/dev/null || true
      fi

      # Check for errors (not just style issues)
      ERROR_COUNT=$(grep -c 'severity="error"' shellcheck.xml 2>/dev/null || echo "0")
      if [[ "$ERROR_COUNT" -gt 0 ]]; then
        echo ""
        echo "Found $ERROR_COUNT error(s) in shell scripts"
        exit 1
      fi

      echo ""
      echo "Lint passed"
  artifacts:
    when: always
    paths:
      - shellcheck.xml
      - shellcheck.html
    expire_in: 1 week
  allow_failure: true
  tags:
    - public-runner-docker

# SQL linting with sqlfluff
# Supports PostgreSQL dialect, outputs YAML report
.haf_app_lint_sql:
  extends: .job-defaults
  stage: build
  image:
    name: sqlfluff/sqlfluff:2.1.4
    entrypoint: [""]  # Override default entrypoint to allow shell scripts
  variables:
    SQLFLUFF_DIALECT: "postgres"
    SQLFLUFF_CONFIG: ""
    SQL_PATHS: "."
    GIT_SUBMODULE_STRATEGY: none
  script:
    - |
      echo "=== Linting SQL Files ==="

      # Build config option if provided
      CONFIG_OPT=""
      if [[ -n "${SQLFLUFF_CONFIG}" ]] && [[ -f "${SQLFLUFF_CONFIG}" ]]; then
        CONFIG_OPT="--config ${SQLFLUFF_CONFIG}"
      fi

      # Run sqlfluff lint
      sqlfluff lint \
        --dialect "${SQLFLUFF_DIALECT}" \
        ${CONFIG_OPT} \
        --format yaml \
        ${SQL_PATHS} 2>&1 | tee sqlfluff.yaml || true

      # Check for violations
      VIOLATION_COUNT=$(grep -c "^-" sqlfluff.yaml 2>/dev/null || echo "0")
      echo ""
      echo "Found $VIOLATION_COUNT violation(s)"

      # Fail on errors (L prefix = errors, not warnings)
      if grep -qE "code: L0[0-9]{2}" sqlfluff.yaml 2>/dev/null; then
        echo "SQL errors found - see sqlfluff.yaml for details"
        exit 1
      fi
  artifacts:
    when: always
    paths:
      - sqlfluff.yaml
    expire_in: 1 week
  allow_failure: true
  tags:
    - public-runner-docker

# =============================================================================
# SERVICE CONTAINER TEMPLATES
# =============================================================================
# Parameterized templates for GitLab CI services section.
# These provide standardized HAF and PostgREST service definitions.
#
# Note: GitLab CI services cannot use !reference directly. Instead, copy
# these definitions into your services section and customize the variables.
#
# Usage example:
#   my_test_job:
#     services:
#       - name: ${HAF_IMAGE_NAME}
#         alias: haf-instance
#         command: ["--stop-at-block=5000024", "--skip-hived"]
#         variables:
#           PG_ACCESS: "host all all 0.0.0.0/0 trust"
#       - name: postgrest/postgrest:v12.0.2
#         alias: postgrest
#         variables:
#           PGRST_DB_URI: "postgres://haf_admin@haf-instance:5432/haf_block_log"
#           PGRST_DB_SCHEMA: "btracker_app"

# HAF instance service configuration (for documentation/reference)
# Copy this pattern into your services section
.haf_service_config:
  variables:
    # PostgreSQL access for HAF container
    HAF_PG_ACCESS: "host all all 0.0.0.0/0 trust"
    # Default HAF block limit for tests
    HAF_STOP_BLOCK: "5000024"
    # Service wait timeout (seconds)
    HAF_SERVICE_TIMEOUT: "300"

# PostgREST service configuration (for documentation/reference)
.postgrest_service_config:
  variables:
    # PostgREST connection settings
    PGRST_DB_URI: "postgres://haf_admin@haf-instance:5432/haf_block_log"
    PGRST_DB_ANON_ROLE: "haf_app_user"
    PGRST_DB_POOL: "10"
    PGRST_SERVER_PORT: "3000"
    # Override PGRST_DB_SCHEMA per-app
    PGRST_DB_SCHEMA: ""

# =============================================================================
# TEST HELPER TEMPLATES
# =============================================================================
# Utility scripts for waiting on services, health checks, etc.
# Use with !reference to compose test job before_script.
#
# Usage:
#   my_test_job:
#     before_script:
#       - !reference [.haf_app_wait_for_postgres, script]
#       - !reference [.haf_app_wait_for_postgrest, script]
#       - # Your additional setup

# Wait for PostgreSQL to be ready (for DinD with docker-compose)
# Requires: docker-compose with 'haf' service, or HAF_PG_HOST variable
.haf_app_wait_for_postgres:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):wait_pg[collapsed=true]\r\e[0KWaiting for PostgreSQL..."

      MAX_WAIT=${HAF_SERVICE_TIMEOUT:-300}
      WAIT_INTERVAL=5
      ELAPSED=0

      # Determine connection method
      if [[ -n "${HAF_PG_HOST:-}" ]]; then
        # Direct connection (service containers)
        PG_CMD="pg_isready -h ${HAF_PG_HOST} -p ${HAF_PG_PORT:-5432} -U ${HAF_PG_USER:-haf_admin}"
      elif command -v docker-compose >/dev/null 2>&1 || command -v docker >/dev/null 2>&1; then
        # Docker compose (DinD)
        PG_CMD="docker compose exec -T haf pg_isready -h localhost -U haf_admin"
      else
        echo "ERROR: No PostgreSQL connection method available"
        exit 1
      fi

      while [[ $ELAPSED -lt $MAX_WAIT ]]; do
        if $PG_CMD >/dev/null 2>&1; then
          echo "PostgreSQL ready after ${ELAPSED}s"
          break
        fi
        echo "Waiting for PostgreSQL... (${ELAPSED}s / ${MAX_WAIT}s)"
        sleep $WAIT_INTERVAL
        ELAPSED=$((ELAPSED + WAIT_INTERVAL))
      done

      if [[ $ELAPSED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgreSQL not ready after ${MAX_WAIT}s"
        exit 1
      fi

      echo -e "\e[0Ksection_end:$(date +%s):wait_pg\r\e[0K"

# Wait for PostgREST to be ready
# Requires: PGRST_HOST or docker-compose with 'postgrest' service
# Note: In DinD, curl may resolve hostnames differently than other tools.
#       This template handles DNS inconsistencies by using getent for resolution.
.haf_app_wait_for_postgrest:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):wait_pgrst[collapsed=true]\r\e[0KWaiting for PostgREST..."

      MAX_WAIT=${PGRST_TIMEOUT:-120}
      WAIT_INTERVAL=3
      ELAPSED=0

      # Determine connection URL
      if [[ -n "${PGRST_HOST:-}" ]]; then
        PGRST_URL="${PGRST_HOST}"
      else
        # In DinD, curl may resolve 'docker' hostname to wrong IP.
        # Use getent to get the correct IP for reliable connections.
        DOCKER_IP=$(getent hosts docker 2>/dev/null | awk '{print $1}' | head -1)
        if [[ -n "$DOCKER_IP" ]]; then
          PGRST_URL="http://${DOCKER_IP}:3000"
          echo "Using resolved Docker IP: $DOCKER_IP"
        else
          PGRST_URL="http://docker:3000"
        fi
      fi
      echo "PostgREST URL: $PGRST_URL"

      while [[ $ELAPSED -lt $MAX_WAIT ]]; do
        if curl -sf --max-time 5 "${PGRST_URL}/" >/dev/null 2>&1; then
          echo "PostgREST ready after ${ELAPSED}s at ${PGRST_URL}"
          break
        fi
        echo "Waiting for PostgREST... (${ELAPSED}s / ${MAX_WAIT}s)"
        sleep $WAIT_INTERVAL
        ELAPSED=$((ELAPSED + WAIT_INTERVAL))
      done

      if [[ $ELAPSED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgREST not ready after ${MAX_WAIT}s"
        echo "Last curl attempt:"
        curl -v --max-time 5 "${PGRST_URL}/" 2>&1 || true
        exit 1
      fi

      echo -e "\e[0Ksection_end:$(date +%s):wait_pgrst\r\e[0K"

# Wait for HAF sync to reach expected block
# Useful for test jobs that need HAF data to be at a specific state
.haf_app_wait_for_sync:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):wait_sync[collapsed=true]\r\e[0KWaiting for HAF sync..."

      TARGET_BLOCK=${HAF_TARGET_BLOCK:-5000000}
      MAX_WAIT=${HAF_SYNC_WAIT_TIMEOUT:-1800}  # 30 minutes default
      WAIT_INTERVAL=10
      ELAPSED=0

      # Build psql command
      if [[ -n "${HAF_PG_HOST:-}" ]]; then
        PSQL_CMD="psql -h ${HAF_PG_HOST} -p ${HAF_PG_PORT:-5432} -U ${HAF_PG_USER:-haf_admin} -d haf_block_log -tAc"
      else
        PSQL_CMD="docker compose exec -T haf psql -U haf_admin -d haf_block_log -tAc"
      fi

      while [[ $ELAPSED -lt $MAX_WAIT ]]; do
        CURRENT_BLOCK=$($PSQL_CMD "SELECT COALESCE(MAX(num), 0) FROM hive.blocks;" 2>/dev/null || echo "0")

        if [[ "$CURRENT_BLOCK" -ge "$TARGET_BLOCK" ]]; then
          echo "HAF synced to block $CURRENT_BLOCK (target: $TARGET_BLOCK) after ${ELAPSED}s"
          break
        fi

        echo "HAF at block $CURRENT_BLOCK / $TARGET_BLOCK... (${ELAPSED}s / ${MAX_WAIT}s)"
        sleep $WAIT_INTERVAL
        ELAPSED=$((ELAPSED + WAIT_INTERVAL))
      done

      if [[ $ELAPSED -ge $MAX_WAIT ]]; then
        echo "ERROR: HAF sync timeout - only at block $CURRENT_BLOCK (target: $TARGET_BLOCK)"
        exit 1
      fi

      echo -e "\e[0Ksection_end:$(date +%s):wait_sync\r\e[0K"

# Extract test cache for DinD jobs
# This is a simplified version of the cache extraction for test jobs
# Requires: APP_SYNC_CACHE_TYPE, APP_CACHE_KEY, HAF_DATA_DIRECTORY
.haf_app_extract_test_cache:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):extract_cache[collapsed=true]\r\e[0KExtracting test cache..."

      # Fetch cache-manager if needed
      if [[ ! -x "$CACHE_MANAGER" ]]; then
        mkdir -p "$(dirname "$CACHE_MANAGER")"
        curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
        chmod +x "$CACHE_MANAGER"
      fi

      # Create target directory
      mkdir -p "${HAF_DATA_DIRECTORY}"

      # Extract cache
      echo "Extracting: ${APP_SYNC_CACHE_TYPE}/${APP_CACHE_KEY} -> ${HAF_DATA_DIRECTORY}"
      if ! CACHE_HANDLING=haf "$CACHE_MANAGER" get "${APP_SYNC_CACHE_TYPE}" "${APP_CACHE_KEY}" "${HAF_DATA_DIRECTORY}"; then
        echo "ERROR: Failed to extract cache"
        exit 1
      fi

      ls -la "${HAF_DATA_DIRECTORY}/"
      echo -e "\e[0Ksection_end:$(date +%s):extract_cache\r\e[0K"

# =============================================================================
# DOCKER COMPOSE TEST ENVIRONMENT TEMPLATES (Phase 4)
# =============================================================================
# Composable templates for setting up and tearing down Docker Compose test
# environments. These handle the common patterns found across HAF app test jobs:
# - Cache extraction with blockchain handling
# - ci.env file creation for docker-compose
# - Service startup and health checks
# - Log collection and cleanup
#
# Usage:
#   my-test-job:
#     extends: .docker_image_builder_job_template
#     before_script:
#       - !reference [.docker_image_builder_job_template, before_script]
#       - !reference [.haf_app_dind_extract_cache, script]
#       - !reference [.haf_app_dind_compose_startup, script]
#       - !reference [.haf_app_dind_wait_for_services, script]
#     script:
#       - # Run your tests
#     after_script:
#       - !reference [.haf_app_dind_compose_teardown, after_script]

# Variables for DinD test jobs
.haf_app_dind_test_variables:
  variables:
    # Job-specific directories (isolated by CI_JOB_ID)
    HAF_DATA_DIRECTORY: "${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir"
    HAF_SHM_DIRECTORY: "${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir"
    # Docker Compose configuration
    COMPOSE_FILE: "docker-compose-test.yml"
    COMPOSE_OPTIONS_STRING: "--file ${COMPOSE_FILE} --ansi never"
    # Service configuration
    HAF_DB_NAME: "haf_block_log"
    HAF_DB_USER: "haf_admin"
    POSTGREST_PORT: "3000"
    # Timeouts
    SERVICE_WAIT_TIMEOUT: "300"

    # ==========================================================================
    # PostgREST Configuration
    # ==========================================================================
    # Apps must set HAF_APP_ROLE_PREFIX to their role prefix (e.g., "btracker").
    # The template automatically constructs the correct PostgREST configuration:
    #   - PGRST_DB_URI uses <prefix>_owner for schema introspection
    #   - PGRST_DB_ANON_ROLE uses <prefix>_user for API request security
    #
    # This pattern is REQUIRED because PostgREST needs owner-level access to
    # introspect the schema (discover functions, parameters, return types),
    # but API requests should run with restricted user permissions.
    #
    # Common mistake (causes "0 Relations" error):
    #   Using <prefix>_user for PGRST_DB_URI - the user role often lacks
    #   permissions to query system catalogs for full schema introspection.
    # ==========================================================================
    HAF_APP_ROLE_PREFIX: ""  # REQUIRED: Set to app role prefix (e.g., "btracker", "reptracker")

    # Optional overrides (defaults derived from HAF_APP_ROLE_PREFIX)
    # PGRST_DB_URI_USER: ""       # Override connection user (default: ${HAF_APP_ROLE_PREFIX}_owner)
    # PGRST_DB_ANON_ROLE: ""      # Override anon role (default: ${HAF_APP_ROLE_PREFIX}_user)
    # PGRST_DB_SCHEMA: ""         # Override schema (default: ${HAF_APP_ROLE_PREFIX}_endpoints)
    # PGRST_DB_EXTRA_SEARCH_PATH: ""  # Override search path (default: ${HAF_APP_ROLE_PREFIX}_app)
    PGRST_DB_POOL: "20"
    PGRST_DB_POOL_ACQUISITION_TIMEOUT: "10"

# Extract cache and prepare blockchain for DinD test jobs
# Handles shm_dir relocation and blockchain symlink/copy logic
# Requires: APP_SYNC_CACHE_TYPE, APP_CACHE_KEY, HAF_DATA_DIRECTORY, HAF_SHM_DIRECTORY
# Use: !reference [.haf_app_dind_extract_cache, script]
.haf_app_dind_extract_cache:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting cache for DinD test..."

      JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"

      # Fetch cache-manager
      if [[ ! -x "$CACHE_MANAGER" ]]; then
        mkdir -p "$(dirname "$CACHE_MANAGER")"
        curl -fsSL "https://gitlab.syncad.com/hive/common-ci-configuration/-/raw/${CACHE_MANAGER_REF:-develop}/scripts/cache-manager.sh" -o "$CACHE_MANAGER"
        chmod +x "$CACHE_MANAGER"
      fi

      echo "Extracting: ${APP_SYNC_CACHE_TYPE}/${APP_CACHE_KEY}"
      mkdir -p "${JOB_DIR}"
      if ! CACHE_HANDLING=haf "$CACHE_MANAGER" get "${APP_SYNC_CACHE_TYPE}" "${APP_CACHE_KEY}" "${JOB_DIR}"; then
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi

      # Relocate shm_dir from inside datadir to be a sibling (for old cache formats)
      if [[ -d "${HAF_DATA_DIRECTORY}/shm_dir" ]] && [[ ! -d "${HAF_SHM_DIRECTORY}" ]]; then
        echo "Relocating shm_dir to parallel location..."
        mv "${HAF_DATA_DIRECTORY}/shm_dir" "${HAF_SHM_DIRECTORY}"
      fi

      # Handle blockchain - copy/symlink to docker directory
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      if [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]] && [[ -n "$(ls -A "${HAF_DATA_DIRECTORY}/blockchain" 2>/dev/null)" ]]; then
        echo "Copying blockchain from cache to docker directory..."
        sudo cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
        sudo rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
      elif [[ -n "${BLOCK_LOG_SOURCE_DIR_5M:-}" ]] && [[ -d "${BLOCK_LOG_SOURCE_DIR_5M}" ]]; then
        echo "Symlinking blockchain from BLOCK_LOG_SOURCE_DIR_5M..."
        ln -sf "${BLOCK_LOG_SOURCE_DIR_5M}/block_log" "${CI_PROJECT_DIR}/docker/blockchain/block_log"
        ln -sf "${BLOCK_LOG_SOURCE_DIR_5M}/block_log.artifacts" "${CI_PROJECT_DIR}/docker/blockchain/block_log.artifacts"
      fi

      ls -la "${HAF_DATA_DIRECTORY}/"
      ls -la "${HAF_SHM_DIRECTORY}/" 2>/dev/null || true
      ls -la "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true

      echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"

# Create ci.env and start Docker Compose services
# Requires: HAF_IMAGE_NAME, HAF_DATA_DIRECTORY, HAF_SHM_DIRECTORY, COMPOSE_OPTIONS_STRING
# Optional: POSTGREST_IMAGE (default: registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest)
#
# PostgREST Configuration:
#   If HAF_APP_ROLE_PREFIX is set, automatically generates correct PostgREST config:
#   - PGRST_DB_URI: Uses <prefix>_owner for connection (schema introspection)
#   - PGRST_DB_ANON_ROLE: Uses <prefix>_user for API requests (security)
#   - PGRST_DB_SCHEMA: Uses <prefix>_endpoints (or override with PGRST_DB_SCHEMA)
#   - PGRST_DB_EXTRA_SEARCH_PATH: Uses <prefix>_app (or override)
#
# Use: !reference [.haf_app_dind_compose_startup, script]
.haf_app_dind_compose_startup:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting Docker Compose services..."

      cd "${CI_PROJECT_DIR}/docker"

      # Validate HAF_APP_ROLE_PREFIX if PostgREST config is needed
      if [[ -n "${HAF_APP_ROLE_PREFIX:-}" ]]; then
        echo "Configuring PostgREST with role prefix: ${HAF_APP_ROLE_PREFIX}"

        # Derive PostgREST settings from role prefix (with optional overrides)
        PGRST_URI_USER="${PGRST_DB_URI_USER:-${HAF_APP_ROLE_PREFIX}_owner}"
        PGRST_ANON="${PGRST_DB_ANON_ROLE:-${HAF_APP_ROLE_PREFIX}_user}"
        PGRST_SCHEMA="${PGRST_DB_SCHEMA:-${HAF_APP_ROLE_PREFIX}_endpoints}"
        PGRST_SEARCH_PATH="${PGRST_DB_EXTRA_SEARCH_PATH:-${HAF_APP_ROLE_PREFIX}_app}"

        echo "  PGRST_DB_URI user: ${PGRST_URI_USER} (for schema introspection)"
        echo "  PGRST_DB_ANON_ROLE: ${PGRST_ANON} (for API requests)"
        echo "  PGRST_DB_SCHEMA: ${PGRST_SCHEMA}"
      else
        echo "WARNING: HAF_APP_ROLE_PREFIX not set - PostgREST config not auto-generated"
        echo "  Set HAF_APP_ROLE_PREFIX to enable automatic PostgREST configuration"
      fi

      # Create ci.env for docker-compose variable substitution
      cat <<-EOF | tee ci.env
      HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
      HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
      HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
      HIVED_UID=$(id -u)
      POSTGREST_IMAGE=${POSTGREST_IMAGE:-registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest}
      EOF

      # Add PostgREST config if role prefix is set
      if [[ -n "${HAF_APP_ROLE_PREFIX:-}" ]]; then
        cat <<-EOF >> ci.env

      # PostgREST configuration (auto-generated from HAF_APP_ROLE_PREFIX=${HAF_APP_ROLE_PREFIX})
      # Pattern: owner role for connection (introspection), user role for API requests
      PGRST_DB_URI=postgresql://${PGRST_URI_USER}@haf:5432/${HAF_DB_NAME:-haf_block_log}
      PGRST_DB_ANON_ROLE=${PGRST_ANON}
      PGRST_DB_SCHEMA=${PGRST_SCHEMA}
      PGRST_DB_POOL=${PGRST_DB_POOL:-20}
      PGRST_DB_POOL_ACQUISITION_TIMEOUT=${PGRST_DB_POOL_ACQUISITION_TIMEOUT:-10}
      PGRST_DB_EXTRA_SEARCH_PATH=${PGRST_SEARCH_PATH}
      EOF
      fi

      # Parse compose options
      IFS=" " read -ra COMPOSE_OPTIONS <<< "${COMPOSE_OPTIONS_STRING}"

      # Validate and start services
      docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
      docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

      echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"

# Wait for PostgreSQL and PostgREST services to be ready
# Includes DNS resolution fix for DinD environments
# Requires: COMPOSE_OPTIONS_STRING, HAF_DB_USER, HAF_DB_NAME, SERVICE_WAIT_TIMEOUT
# Optional: HAF_APP_SCHEMA (to verify schema exists)
# Use: !reference [.haf_app_dind_wait_for_services, script]
.haf_app_dind_wait_for_services:
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services..."

      cd "${CI_PROJECT_DIR}/docker"
      IFS=" " read -ra COMPOSE_OPTIONS <<< "${COMPOSE_OPTIONS_STRING}"
      MAX_WAIT="${SERVICE_WAIT_TIMEOUT:-300}"
      WAITED=0

      # Wait for PostgreSQL via docker compose exec
      until docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf pg_isready -U "${HAF_DB_USER:-haf_admin}" -d "${HAF_DB_NAME:-haf_block_log}" 2>/dev/null; do
        echo "Waiting for PostgreSQL... ($WAITED/$MAX_WAIT seconds)"
        sleep 5
        WAITED=$((WAITED + 5))
        if [[ $WAITED -ge $MAX_WAIT ]]; then
          echo "ERROR: PostgreSQL did not become ready in time"
          docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf | tail -50
          exit 1
        fi
      done
      echo "PostgreSQL ready (took ${WAITED}s)"

      # Verify app schema if specified
      if [[ -n "${HAF_APP_SCHEMA:-}" ]]; then
        if ! docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U "${HAF_DB_USER:-haf_admin}" -d "${HAF_DB_NAME:-haf_block_log}" -t -c "SELECT 1 FROM information_schema.schemata WHERE schema_name = '${HAF_APP_SCHEMA}'" | grep -q 1; then
          echo "ERROR: ${HAF_APP_SCHEMA} schema not found"
          exit 1
        fi
        echo "${HAF_APP_SCHEMA} schema verified"
      fi

      # Wait for PostgREST with DNS resolution fix
      # In DinD, curl may resolve 'docker' to wrong IP - use getent for reliable resolution
      DOCKER_IP=$(getent hosts docker 2>/dev/null | awk '{print $1}' | head -1)
      if [[ -z "$DOCKER_IP" ]]; then
        DOCKER_IP="${DOCKER_HOST:-docker}"
        DOCKER_IP="${DOCKER_IP#tcp://}"
        DOCKER_IP="${DOCKER_IP%%:*}"
      fi
      echo "Using Docker host IP: $DOCKER_IP for PostgREST"

      WAITED=0
      until curl -sf --max-time 5 "http://${DOCKER_IP}:${POSTGREST_PORT:-3000}/" >/dev/null 2>&1; do
        echo "Waiting for PostgREST at ${DOCKER_IP}:${POSTGREST_PORT:-3000}... ($WAITED/$MAX_WAIT seconds)"
        sleep 5
        WAITED=$((WAITED + 5))
        if [[ $WAITED -ge $MAX_WAIT ]]; then
          echo "ERROR: PostgREST did not become ready in time"
          docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest 2>/dev/null | tail -50 || true
          exit 1
        fi
      done
      echo "PostgREST ready at ${DOCKER_IP}:${POSTGREST_PORT:-3000}"

      # Export for test scripts
      export POSTGREST_HOST="${DOCKER_IP}"

      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"

# Teardown Docker Compose and collect logs
# Use in after_script: !reference [.haf_app_dind_compose_teardown, after_script]
.haf_app_dind_compose_teardown:
  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up Docker Compose..."

      cd "${CI_PROJECT_DIR}/docker" 2>/dev/null || exit 0

      # Parse compose options (use default if variable not set)
      IFS=" " read -ra COMPOSE_OPTIONS <<< "${COMPOSE_OPTIONS_STRING:---file docker-compose-test.yml --ansi never}"

      # Collect logs before teardown
      docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true
      docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest > postgrest.log 2>&1 || true

      # Stop and remove containers
      docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" down --volumes --remove-orphans || true

      # Archive logs
      tar -czvf container-logs.tar.gz *.log 2>/dev/null || true

      # Clean up job-specific data directory
      sudo rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || true

      echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"

# =============================================================================
# COMPLETE DIND TEST JOB TEMPLATE (Phase 4)
# =============================================================================
# A complete, ready-to-use test job template that combines all DinD components.
# Apps only need to provide their test script.
#
# Required variables (set in your job):
#   APP_SYNC_CACHE_TYPE: Cache type (e.g., "haf_btracker_sync")
#   APP_CACHE_KEY: Cache key (e.g., "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}")
#   HAF_IMAGE_NAME: HAF Docker image (from prepare_haf_image job)
#
# PostgREST configuration (recommended):
#   HAF_APP_ROLE_PREFIX: App's database role prefix (e.g., "btracker", "reptracker")
#     When set, automatically generates correct PostgREST config in ci.env:
#     - PGRST_DB_URI: postgresql://<prefix>_owner@haf:5432/haf_block_log
#     - PGRST_DB_ANON_ROLE: <prefix>_user
#     - PGRST_DB_SCHEMA: <prefix>_endpoints
#     Your docker-compose-test.yml should use ${PGRST_*} variable references.
#
# Optional variables:
#   HAF_APP_SCHEMA: Schema to verify exists (e.g., "btracker_app")
#   COMPOSE_FILE: Docker Compose file (default: docker-compose-test.yml)
#   POSTGREST_IMAGE: PostgREST image (default: registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest)
#   PGRST_DB_SCHEMA: Override endpoint schema (default: ${HAF_APP_ROLE_PREFIX}_endpoints)
#   PGRST_DB_EXTRA_SEARCH_PATH: Override search path (default: ${HAF_APP_ROLE_PREFIX}_app)
#
# Usage:
#   my-test:
#     extends: .haf_app_dind_complete_test
#     needs:
#       - sync
#       - prepare_haf_image
#     variables:
#       APP_SYNC_CACHE_TYPE: "haf_myapp_sync"
#       APP_CACHE_KEY: "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}"
#       HAF_APP_ROLE_PREFIX: "myapp"  # Auto-configures PostgREST
#       HAF_APP_SCHEMA: "myapp_app"
#     script:
#       - ./run-my-tests.sh --host=docker
#
.haf_app_dind_complete_test:
  extends:
    - .docker_image_builder_job_template
    - .haf_app_dind_test_variables
  stage: test
  timeout: 30 minutes
  before_script:
    - !reference [.docker_image_builder_job_template, before_script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    - !reference [.haf_app_dind_extract_cache, script]
    - !reference [.haf_app_dind_compose_startup, script]
    - !reference [.haf_app_dind_wait_for_services, script]
  after_script: !reference [.haf_app_dind_compose_teardown, after_script]
  artifacts:
    when: always
    paths:
      - docker/container-logs.tar.gz
    expire_in: 1 week
  tags:
    - data-cache-storage
    - fast

# =============================================================================
# COMPLETE PREPARE_HAF_DATA TEMPLATE WITH STANDARD NEEDS
# =============================================================================
# This template extends .prepare_haf_data_5m and adds the standard needs
# dependency on find_haf_image. Use this to avoid duplicating the needs:
# definition across repos.
#
# All HAF apps should use find_haf_image to dynamically detect the latest
# HAF image from the registry, eliminating the need for submodules or
# manual version tracking.

# Template for repos that use pre-built HAF images from registry
# Use when you have: find_haf_image -> prepare_haf_data
#
# Usage:
#   prepare_haf_data:
#     extends: .haf_app_prepare_data_with_find
#     stage: build
#     variables:
#       BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
#       HAF_IMAGE_NAME: "${HAF_UPSTREAM_IMAGE}"
#       HAF_COMMIT: "${HAF_UPSTREAM_COMMIT}"
#     tags:
#       - data-cache-storage
#
.haf_app_prepare_data_with_find:
  extends: .prepare_haf_data_5m
  needs:
    - job: find_haf_image
      artifacts: true

# =============================================================================
# REUSABLE NEEDS FRAGMENTS
# =============================================================================
# For jobs that need to add detect_changes as an optional dependency alongside
# other custom dependencies, use !reference to include this fragment.
#
# Usage:
#   sync:
#     needs:
#       - !reference [.needs_detect_changes_optional, needs, 0]
#       - job: find_haf_image
#         artifacts: true
#       - job: my_custom_job
#
.needs_detect_changes_optional:
  needs:
    - job: detect_changes
      artifacts: true
      optional: true  # For tag pipelines where detect_changes is skipped
